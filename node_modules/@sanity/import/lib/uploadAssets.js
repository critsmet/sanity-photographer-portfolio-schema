"use strict";

function _slicedToArray(arr, i) { return _arrayWithHoles(arr) || _iterableToArrayLimit(arr, i) || _nonIterableRest(); }

function _nonIterableRest() { throw new TypeError("Invalid attempt to destructure non-iterable instance"); }

function _iterableToArrayLimit(arr, i) { var _arr = []; var _n = true; var _d = false; var _e = undefined; try { for (var _i = arr[Symbol.iterator](), _s; !(_n = (_s = _i.next()).done); _n = true) { _arr.push(_s.value); if (i && _arr.length === i) break; } } catch (err) { _d = true; _e = err; } finally { try { if (!_n && _i["return"] != null) _i["return"](); } finally { if (_d) throw _e; } } return _arr; }

function _arrayWithHoles(arr) { if (Array.isArray(arr)) return arr; }

function asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { Promise.resolve(value).then(_next, _throw); } }

function _asyncToGenerator(fn) { return function () { var self = this, args = arguments; return new Promise(function (resolve, reject) { var gen = fn.apply(self, args); function _next(value) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, "next", value); } function _throw(err) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, "throw", err); } _next(undefined); }); }; }

const basename = require('path').basename;

const parseUrl = require('url').parse;

const debug = require('debug')('sanity:import');

const pMap = require('p-map');

const progressStepper = require('./util/progressStepper');

const getHashedBufferForUri = require('./util/getHashedBufferForUri');

const retryOnFailure = require('./util/retryOnFailure');

const ASSET_UPLOAD_CONCURRENCY = 3;
const ASSET_PATCH_CONCURRENCY = 3;
const ASSET_PATCH_BATCH_SIZE = 50;

function uploadAssets(_x, _x2) {
  return _uploadAssets.apply(this, arguments);
}

function _uploadAssets() {
  _uploadAssets = _asyncToGenerator(function* (assets, options) {
    // Build a Map where the keys are `type#url` and the value is an array of all
    // objects containing document id and path to inject asset reference to.
    // `assets` is an array of objects with shape: {documentId, path, url, type}
    const assetRefMap = getAssetRefMap(assets) // We might have additional assets that is not referenced by any documents, but was part of a
    // dataset when exporting, for instance. Add these to the map without any references to update.
    ;
    (options.unreferencedAssets || []).forEach(asset => {
      if (!assetRefMap.has(asset)) {
        assetRefMap.set(asset, []);
      }
    }); // Create a function we can call for every completed upload to report progress

    const progress = progressStepper(options.onProgress, {
      step: 'Importing assets (files/images)',
      total: assetRefMap.size
    }); // Loop over all unique URLs and ensure they exist, and if not, upload them

    const mapOptions = {
      concurrency: ASSET_UPLOAD_CONCURRENCY
    };
    const assetIds = yield pMap(assetRefMap.keys(), ensureAssetWithRetries.bind(null, options, progress), mapOptions); // Loop over all documents that need asset references to be set

    const batches = yield setAssetReferences(assetRefMap, assetIds, options);
    return batches.reduce((prev, add) => prev + add, 0);
  });
  return _uploadAssets.apply(this, arguments);
}

function getAssetRefMap(assets) {
  return assets.reduce((assetRefMap, item) => {
    const documentId = item.documentId,
          path = item.path,
          url = item.url,
          type = item.type;
    const key = `${type}#${url}`;
    let refs = assetRefMap.get(key);

    if (!refs) {
      refs = [];
      assetRefMap.set(key, refs);
    }

    refs.push({
      documentId,
      path
    });
    return assetRefMap;
  }, new Map());
}

function ensureAssetWithRetries(...args) {
  return retryOnFailure(() => ensureAsset(...args));
}

function ensureAsset(_x3, _x4, _x5, _x6) {
  return _ensureAsset.apply(this, arguments);
}

function _ensureAsset() {
  _ensureAsset = _asyncToGenerator(function* (options, progress, assetKey, i) {
    const client = options.client,
          _options$assetMap = options.assetMap,
          assetMap = _options$assetMap === void 0 ? {} : _options$assetMap;

    const _assetKey$split = assetKey.split('#', 2),
          _assetKey$split2 = _slicedToArray(_assetKey$split, 2),
          type = _assetKey$split2[0],
          url = _assetKey$split2[1]; // Download the asset in order for us to create a hash


    debug('[Asset #%d] Downloading %s', i, url);

    const _ref = yield getHashedBufferForUri(url),
          buffer = _ref.buffer,
          sha1hash = _ref.sha1hash; // See if the item exists on the server


    debug('[Asset #%d] Checking for asset with hash %s', i, sha1hash);
    const assetDocId = yield getAssetDocumentIdForHash(client, type, sha1hash);

    if (assetDocId) {
      // Same hash means we want to reuse the asset
      debug('[Asset #%d] Found %s for hash %s', i, type, sha1hash);
      progress();
      return assetDocId;
    }

    const assetMeta = assetMap[`${type}-${sha1hash}`];
    const hasFilename = assetMeta && assetMeta.originalFilename;
    const hasNonFilenameMeta = assetMeta && Object.keys(assetMap).length > 1;

    const _parseUrl = parseUrl(url),
          pathname = _parseUrl.pathname;

    const filename = hasFilename ? assetMeta.originalFilename : basename(pathname); // If it doesn't exist, we want to upload it

    debug('[Asset #%d] Uploading %s with URL %s', i, type, url);
    const asset = yield client.assets.upload(type, buffer, {
      filename
    });
    progress(); // If we have more metadata to provide, update the asset document

    if (hasNonFilenameMeta) {
      yield client.patch(asset._id).set(assetMeta);
    }

    return asset._id;
  });
  return _ensureAsset.apply(this, arguments);
}

function getAssetDocumentIdForHash(_x7, _x8, _x9) {
  return _getAssetDocumentIdForHash.apply(this, arguments);
}

function _getAssetDocumentIdForHash() {
  _getAssetDocumentIdForHash = _asyncToGenerator(function* (client, type, sha1hash, attemptNum = 0) {
    // @todo remove retry logic when client has reintroduced it
    try {
      const dataType = type === 'file' ? 'sanity.fileAsset' : 'sanity.imageAsset';
      const query = '*[_type == $dataType && sha1hash == $sha1hash][0]._id';
      const assetDocId = yield client.fetch(query, {
        dataType,
        sha1hash
      });
      return assetDocId;
    } catch (err) {
      if (attemptNum < 3) {
        return getAssetDocumentIdForHash(client, type, sha1hash, attemptNum + 1);
      }

      err.attempts = attemptNum;
      throw new Error(`Error while attempt to query Sanity API:\n${err.message}`);
    }
  });
  return _getAssetDocumentIdForHash.apply(this, arguments);
}

function setAssetReferences(assetRefMap, assetIds, options) {
  const client = options.client;
  const lookup = assetRefMap.values();
  const patchTasks = assetIds.reduce((tasks, assetId) => {
    const documents = lookup.next().value;
    return tasks.concat(documents.map(({
      documentId,
      path
    }) => ({
      documentId,
      path,
      assetId
    })));
  }, []); // We now have an array of simple tasks, each containing:
  // {documentId, path, assetId}
  // Instead of doing a single mutation per asset, let's batch them up

  const batches = [];

  for (let i = 0; i < patchTasks.length; i += ASSET_PATCH_BATCH_SIZE) {
    batches.push(patchTasks.slice(i, i + ASSET_PATCH_BATCH_SIZE));
  } // Since separate progress step for batches of reference sets


  const progress = progressStepper(options.onProgress, {
    step: 'Setting asset references to documents',
    total: batches.length
  }); // Now perform the batch operations in parallel with a given concurrency

  const mapOptions = {
    concurrency: ASSET_PATCH_CONCURRENCY
  };
  return pMap(batches, setAssetReferenceBatch.bind(null, client, progress), mapOptions);
}

function setAssetReferenceBatch(client, progress, batch) {
  debug('Setting asset references on %d documents', batch.length);
  return retryOnFailure(() => batch.reduce(reducePatch, client.transaction()).commit({
    visibility: 'async'
  }).then(progress).then(res => res.results.length));
}

function getAssetType(assetId) {
  return assetId.slice(0, assetId.indexOf('-'));
}

function reducePatch(trx, task) {
  return trx.patch(task.documentId, patch => patch.set({
    [`${task.path}._type`]: getAssetType(task.assetId),
    [`${task.path}.asset`]: {
      _type: 'reference',
      _ref: task.assetId
    }
  }));
}

module.exports = uploadAssets;